{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcoXVCnr2rp7e/Oj40MPc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzaja123/titanic-dataset-CPU-GPU-TPU-evaluation/blob/main/titanic_dataset_CPU_GPU_TPU_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "pqOqfr-mHki1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data Function\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "    except:\n",
        "        print(\"File not found\")\n",
        "        data = None\n",
        "    return data"
      ],
      "metadata": {
        "id": "CPnI4DnEH4_O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Data Function\n",
        "def preprocess_data(data, is_test=False):\n",
        "    # Preprocessing\n",
        "    gender_factorized = data.copy()\n",
        "\n",
        "    # Factorize 'Sex'\n",
        "    gender_factorized['Sex'] = gender_factorized['Sex'].replace(['male', 'female'], [0, 1])\n",
        "\n",
        "    # Factorize 'Embarked'\n",
        "    gender_factorized['Embarked'] = gender_factorized['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2])\n",
        "\n",
        "    if not is_test:\n",
        "        # Select features for training data\n",
        "        gender_factorized['Survived'] = gender_factorized['Survived'].astype(int)\n",
        "        feature = gender_factorized[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "        feature = feature.dropna()\n",
        "        return feature\n",
        "    else:\n",
        "        # Select features for test data\n",
        "        feature = gender_factorized[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "        feature['Fare'].replace(np.nan, feature['Fare'].median(), inplace=True)\n",
        "        feature['Age'].replace(np.nan, feature['Age'].median(), inplace=True)\n",
        "        # Add a placeholder for 'Survived' column (not used in predictions)\n",
        "        feature.loc[:, 'Survived'] = 0\n",
        "        return feature"
      ],
      "metadata": {
        "id": "QYm-6PNoH6t-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data Function\n",
        "def split_data(feature_data):\n",
        "    # Splitting Data\n",
        "    def split_data_frame(df, frac, axis=0) -> list:\n",
        "        if axis == 0:\n",
        "            threshold = int(df.shape[0] * frac)\n",
        "            part1 = df.iloc[0: threshold, :].reset_index(drop=True)\n",
        "            part2 = df.iloc[threshold:, :].reset_index(drop=True)\n",
        "        elif axis == 1:\n",
        "            threshold = df.shape[1] * frac\n",
        "            part1 = df.iloc[:, 0: threshold].reset_index(drop=True)\n",
        "            part2 = df.iloc[:, threshold:].reset_index(drop=True)\n",
        "        else:\n",
        "            print('Key \"axis\" is \"0\" or \"1\"')\n",
        "            return [None, None]\n",
        "        return [part1, part2]\n",
        "\n",
        "    splitted = split_data_frame(feature_data, 0.8)\n",
        "    train_data = np.array(splitted[0].iloc[:, 1:])\n",
        "    train_label = np.array(splitted[0].iloc[:, 0])\n",
        "    validation_data = np.array(splitted[1].iloc[:, 1:])\n",
        "    validation_label = np.array(splitted[1].iloc[:, 0])\n",
        "    return train_data, train_label, validation_data, validation_label"
      ],
      "metadata": {
        "id": "7oTmQv4cH9A2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model Function\n",
        "def build_model():\n",
        "    # Building Model\n",
        "    model = Sequential()\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "uxuQ0X96H-99"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate on CPU Function\n",
        "def train_evaluate_cpu(train_data, train_label, validation_data, validation_label, epochs=500):\n",
        "    print(\"Training and evaluating on CPU:\")\n",
        "    model_cpu = build_model()\n",
        "    acc, loss = train_model(model_cpu, train_data, train_label, epochs)\n",
        "    print(\"Training Accuracy:\", acc[-1])\n",
        "    print(\"Training Loss:\", loss[-1])\n",
        "\n",
        "    result = evaluate_model(model_cpu, validation_data, validation_label)\n",
        "    print(\"Validation Accuracy:\", result[1])\n",
        "    print(\"Validation Loss:\", result[0])\n",
        "    print(\"------------------------- \\n\")\n",
        "    return model_cpu"
      ],
      "metadata": {
        "id": "Xxpr-hY9IApu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate on GPU Function\n",
        "def train_evaluate_gpu(train_data, train_label, validation_data, validation_label, epochs=500):\n",
        "    print(\"Training and evaluating on GPU:\")\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        model_gpu = build_model()\n",
        "        acc_gpu, loss_gpu = train_model(model_gpu, train_data, train_label, epochs)\n",
        "        print(\"Training Accuracy:\", acc_gpu[-1])\n",
        "        print(\"Training Loss:\", loss_gpu[-1])\n",
        "\n",
        "        result_gpu = evaluate_model(model_gpu, validation_data, validation_label)\n",
        "        print(\"Validation Accuracy:\", result_gpu[1])\n",
        "        print(\"Validation Loss:\", result_gpu[0])\n",
        "        print(\"------------------------- \\n\")\n",
        "        return model_gpu"
      ],
      "metadata": {
        "id": "T1cPHjxEIT0G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate on TPU Function\n",
        "def train_evaluate_tpu(train_data, train_label, validation_data, validation_label, epochs=500):\n",
        "    print(\"Training and evaluating on TPU:\")\n",
        "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
        "\n",
        "    with strategy.scope():\n",
        "        model_tpu = build_model()\n",
        "        acc_tpu, loss_tpu = train_model(model_tpu, train_data, train_label, epochs)\n",
        "        print(\"Training Accuracy:\", acc_tpu[-1])\n",
        "        print(\"Training Loss:\", loss_tpu[-1])\n",
        "\n",
        "        result_tpu = evaluate_model(model_tpu, validation_data, validation_label)\n",
        "        print(\"Validation Accuracy:\", result_tpu[1])\n",
        "        print(\"Validation Loss:\", result_tpu[0])\n",
        "        print(\"------------------------- \\n\")\n",
        "        return model_tpu"
      ],
      "metadata": {
        "id": "F1PTkJjcIVi-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model Function\n",
        "def train_model(model, train_data, train_label, epochs=500):\n",
        "    history = model.fit(train_data, train_label, epochs=epochs, verbose=0)\n",
        "    acc = history.history['accuracy']\n",
        "    loss = history.history['loss']\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "UaNEG15CIXLF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Function\n",
        "def evaluate_model(model, validation_data, validation_label):\n",
        "    result = model.evaluate(validation_data, validation_label)\n",
        "    return result"
      ],
      "metadata": {
        "id": "sjS8sJ53IZHN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict Data Function\n",
        "def predict_data(model, test_data):\n",
        "    predict = model.predict(test_data)\n",
        "    binary_result = (predict > 0.5).astype(int)\n",
        "    binary_result = binary_result.reshape(test_data.shape[0])\n",
        "    return binary_result"
      ],
      "metadata": {
        "id": "ogEhz_pBIaUu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction:**\n",
        "In order to optimize the performance of your notebook, it is crucial to define the hardware accelerator in the notebook settings. This step allows you to harness the power of GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units) for faster computations, especially beneficial for machine learning tasks.\n",
        "\n",
        "**Instructions:**\n",
        "Follow the steps below to set up the hardware accelerator in Google Colab:\n",
        "\n",
        "- Click on the \"Edit -> Notebook settings\" option in the Colab toolbar.\n",
        "- Choose the desired accelerator from the \"Hardware accelerator\" (CPU, GPU or TPU).\n",
        "- Click \"Save\" to apply the changes.\n",
        "\n",
        "**Important Note:**\n",
        "After modifying the hardware accelerator, it is necessary to reload the dataset files. If you have previously uploaded the dataset files (test.csv and train.csv), please re-upload them to ensure compatibility with the selected hardware accelerator.\n",
        "\n",
        "These adjustments will enhance the speed and efficiency of your notebook, making it ideal for resource-intensive tasks such as machine learning model training and evaluation."
      ],
      "metadata": {
        "id": "GHELV5zgJn0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'train.csv' and the 'test.csv' files from the github repo\n",
        "#import subprocess\n",
        "#subprocess.run(['wget', 'https://raw.githubusercontent.com/your_username/your_repo/main/train.csv', '-O', 'train.csv'])\n",
        "#subprocess.run(['wget', 'https://raw.githubusercontent.com/your_username/your_repo/main/test.csv', '-O', 'test.csv'])"
      ],
      "metadata": {
        "id": "RjO6Sn9vP9vt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "def main():\n",
        "    # Load Data\n",
        "    train_data_path = 'train.csv'\n",
        "    test_data_path = 'test.csv'\n",
        "    train_data = load_data(train_data_path)\n",
        "    test_data = load_data(test_data_path)\n",
        "\n",
        "    # Preprocess Train Data\n",
        "    feature_data = preprocess_data(train_data)\n",
        "\n",
        "    # Split Data\n",
        "    train_data, train_label, validation_data, validation_label = split_data(feature_data)\n",
        "\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        in_colab = \"google.colab\" in str(get_ipython())\n",
        "    except:\n",
        "        print(\"Runing the code locally.\")\n",
        "        in_colab = False\n",
        "\n",
        "    # Train and Evaluate based on the available device\n",
        "    if in_colab and (tf.test.gpu_device_name() == \"/device:GPU:0\"):\n",
        "        # Train and Evaluate on GPU\n",
        "        model = train_evaluate_gpu(train_data, train_label, validation_data, validation_label)\n",
        "    elif in_colab and (\"COLAB_TPU_ADDR\" in os.environ):\n",
        "        # Train and Evaluate on TPU\n",
        "        model = train_evaluate_tpu(train_data, train_label, validation_data, validation_label)\n",
        "    else:\n",
        "        # Train and Evaluate on CPU\n",
        "        model = train_evaluate_cpu(train_data, train_label, validation_data, validation_label)\n",
        "\n",
        "    # Preprocess Test Data\n",
        "    test_data_processed = preprocess_data(test_data, is_test=True)\n",
        "    test_data_array = np.array(test_data_processed.iloc[:, 1:])\n",
        "\n",
        "    # Prediction\n",
        "    binary_result = predict_data(model, test_data_array)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Predictions: \\n\", binary_result)\n",
        "\n",
        "    # Clear the session to release resources\n",
        "    keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "iDJDdDuKIbvm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore the pandas \"depricated \"warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Md4iNdmVMO0N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the full code, use the \"Runtime -> Run all\" option from the option menu, or use the Ctrl+F9 command from the keyboard."
      ],
      "metadata": {
        "id": "YJFg06B3LI5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGZVPIFHI5x-",
        "outputId": "6d553921-6e93-460a-abd7-b2c8d79635ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating on TPU:\n",
            "Training Accuracy: 0.7328646779060364\n",
            "Training Loss: 0.539980947971344\n",
            "5/5 [==============================] - 2s 78ms/step - loss: 0.4382 - accuracy: 0.7902\n",
            "Validation Accuracy: 0.7902097702026367\n",
            "Validation Loss: 0.4381600320339203\n",
            "------------------------- \n",
            "\n",
            "14/14 [==============================] - 2s 35ms/step\n",
            "Predictions: \n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    }
  ]
}